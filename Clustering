import networkx as nx
import matplotlib.pyplot as plt

# """
# Clustering the datasets using the Louvain method
# """

#It creates a Graph element starting from a list of weighted edges
def GraphCreation(filepath):
  #reads the file
  with open(filepath,"r") as file:
    content=file.read()
  
  #from txt to vector
  num=''
  vector=list()
  for line in content:
    if (line!=' ' and line!='\n'):
      num=num+line
    else:
      vector.append(int(num))
      num=''
  
  #saves the edges and the weigths 
  nodes_1=vector[::3]
  nodes_2=vector[1::3]
  weigths=vector[2::3]
  
  #creates the graph
  nodes=list(set(nodes_1+nodes_2))
  edges=list()
   
  for i in range(1,len(nodes_1)+1):
    temp_edge=(nodes_1[i-1], nodes_2[i-1], weigths[i-1])
    edges.append(temp_edge)
  
  G=nx.Graph()
  G.add_nodes_from(nodes)
  G.add_weighted_edges_from(edges)
  return G

#Starting from a graph, it applies the louvain algorithm method to cluster it  
def Clustering(G):
  #Initialization of the quality score and the community vector
  quality=0
  com_fin=list()
  
  #Let's run 100 times the louvain clustering algorithm and save the best community and its quality score
  for i in range(1, 100):
    com=nx.community.louvain_communities(G, weight='weight')
    if quality<nx.community.modularity(G, com):
      quality=nx.community.modularity(G,com)
      com_fin=com
  #l contains, for each community, the number of points in that community
  l=[len(i) for i in com_fin]
  clust=[com_fin,quality,l]
  return clust

# Starting from a graph, a community and a paritition of the nodes, this function
# create a new graph element where the nodes are the communities and the edges are the
# (intra and extra) cluster edges of the original graph. The weghts associated to these are
# the sum of all the weigths of the edges that link the same communities in the
# original graph.
def new_network(G, community, par):
  nodes=list()
  edges=list()
  edges_w=list()
  
  for i in par:
    nodes.append(community[min(i)])
  
  G1=nx.Graph()
  G1.add_nodes_from(nodes)
  
  for edge in G.edges():
    new_edge=[min(community[edge[0]],community[edge[1]]),max(community[edge[0]],community[edge[1]])]
    if new_edge in edges:
      #I've to find the arc element to modify the weight
      index_found=None
      for index,arc in enumerate(edges_w):
        if arc[0]==new_edge[0] and arc[1]==new_edge[1]:
          index_found=index
          break
      #Modify the weight
      weight_edge=G.get_edge_data(*edge)['weight']
      edges_w[index][2]+=weight_edge
    else:
      #add new edges and weighted edges
      edges.append(new_edge)
      weight_edge=G.get_edge_data(*edge)['weight']
      edges_w.append([*new_edge, weight_edge])
  G1.add_weighted_edges_from(edges_w)
  return G1

# From a partition of the graph (a list of set, in which there are the nodes of a community),
# this function creates a dictionary where all the keys are the nodes, all the values are the
# number representing the community of the specific node
def par2com(par):
  com={}
  for i in range(0, len(par)):
    for element in par[i]:
      com[element]=i
  return com

# Given a graph and its community division, this function change the values of the 
# community dictionary and put the label of the job done by the associated node.
def work_assign(G, community):
  
  #Analyze the stats of the reduced graph and find the nurses and the cleaners
  intra_weights={}
  for edge in G.edges():
    if edge[0]==edge[1]:
      intra_weights[edge[0]]=G.get_edge_data(*edge)['weight']
  
  # The nurses are the ones with more intra comunication
  nurses=max(intra_weights, key=lambda k: intra_weights[k])
  
  # The cleaners are the ones with less intra comunication
  cleaners=min(intra_weights, key=lambda k: intra_weights[k])
  
  # print('cleaners', cleaners)
  # print('nurses', nurses)
  
  # Assign the work to the nodes
  for node in community.keys():
    if community[node]==nurses:
      community[node]='Nurse'
    elif community[node]==cleaners:
      community[node]='Cleaner'
    else:
      community[node]='Forager'

  return community

# Given two communities (dictionary with keys-nodes and values-work labels) it scans
# for each node if its work is changed and saves the change in a a specific vector
def work_changes(com1, com2):
  #Initialization of the counters
  nurs2clean=0
  nurs2for=0
  for2nurs=0
  clean2nurs=0
  clean2for=0
  for2clean=0
  
  #for each node, it compare the jobs and saves the changes
  for node in com1.keys():
    if com1[node]=='Nurse' and com2[node]=='Cleaner': nurs2clean+=1
    elif com1[node]=='Nurse' and com2[node]=='Forager': nurs2for+=1
    elif com1[node]=='Cleaner' and com2[node]=='Nurse': clean2nurs+=1
    elif com1[node]=='Cleaner' and com2[node]=='Forager': clean2for+=1
    elif com1[node]=='Forager' and com2[node]=='Nurse': for2nurs+=1
    elif com1[node]=='Forager' and com2[node]=='Cleaner': for2clean+=1
    
  changes=[nurs2clean,nurs2for,clean2nurs,clean2for,for2nurs,for2clean]
  return changes

# Taking a Graph and a community with work lables, this functions identifies the node 
#with the highest degree in the community of 'Nurses', and returns it with its degree value
def find_queen(com, G):
  max_d=0
  queen=0
  for node in com.keys():
    if com[node]=='Nurse' and G.degree[node]>max_d:
      max_d=G.degree[node]
      queen=node
  return [queen, max_d]

# Given a list of three nodes and 3 graphs, this functions find which node has the highest
# mean value of degree in the 3 different graphs
def queen_comp(queens, g1, g2, g3):
  queen1=0
  max_d=0
  for queen in queens:
    d=(g1.degree[queen]+g2.degree[queen]+g3.degree[queen])/3
    if d>max_d:
      max_d=d
      queen1=queen
  return queen1



#############################################################################################
#Starting from 3 different files, it creates 3 graphs and compute the communities in these.
#At the end:
#ColonyX_dayX_Graph is the graph obtained from the specific colony
#ColonyX_dayX_Clust is a vector containing the partition of the nodes, the quality score of the clustering and the number of nodes of each community
#par_colonyX is a dictionary where the keys are the days and the values are the partitions of the nodes      
###########################             COLONY 1              ##############################

par_colony1={}

file1=""
Colony1_day1_Graph=GraphCreation(file1)
Colony1_day1_Clust=Clustering(Colony1_day1_Graph)
print(Colony1_day1_Clust[1],sorted(Colony1_day1_Clust[2]))
par_colony1['day 1']=Colony1_day1_Clust[0]

file2=""
Colony1_day5_Graph=GraphCreation(file2)
Colony1_day5_Clust=Clustering(Colony1_day5_Graph)
print(Colony1_day5_Clust[1],sorted(Colony1_day5_Clust[2]))
par_colony1['day 5']=Colony1_day5_Clust[0]

file3=""
Colony1_day10_Graph=GraphCreation(file3)
Colony1_day10_Clust=Clustering(Colony1_day10_Graph)
print(Colony1_day10_Clust[1],sorted(Colony1_day10_Clust[2]))
par_colony1['day 10']=Colony1_day10_Clust[0]

###########################             COLONY 2              ##############################

par_colony2={}

file4=""
Colony2_day1_Graph=GraphCreation(file4)
Colony2_day1_Clust=Clustering(Colony2_day1_Graph)
print(Colony2_day1_Clust[1],sorted(Colony2_day1_Clust[2]))
par_colony2['day 1']=Colony2_day1_Clust[0]

file5=""
Colony2_day5_Graph=GraphCreation(file5)
Colony2_day5_Clust=Clustering(Colony2_day5_Graph)
print(Colony2_day5_Clust[1],sorted(Colony2_day5_Clust[2]))
par_colony2['day 5']=Colony2_day5_Clust[0]

file6=""
Colony2_day10_Graph=GraphCreation(file6)
Colony2_day10_Clust=Clustering(Colony2_day10_Graph)
print(Colony2_day10_Clust[1],sorted(Colony2_day10_Clust[2]))
par_colony2['day 10']=Colony2_day10_Clust[0]

###########################             COLONY 3              ##############################

par_colony3={}

file7=""
Colony3_day1_Graph=GraphCreation(file7)
Colony3_day1_Clust=Clustering(Colony3_day1_Graph)
print(Colony3_day1_Clust[1],sorted(Colony3_day1_Clust[2]))
par_colony3['day 1']=Colony3_day1_Clust[0]

file8=""
Colony3_day5_Graph=GraphCreation(file8)
Colony3_day5_Clust=Clustering(Colony3_day5_Graph)
print(Colony3_day5_Clust[1],sorted(Colony3_day5_Clust[2]))
par_colony3['day 5']=Colony3_day5_Clust[0]

file9="C:\\Users\\alelu\\Desktop\\Ant Colony\\Datasets\\Colony3-day09.txt"
Colony3_day10_Graph=GraphCreation(file9)
Colony3_day10_Clust=Clustering(Colony3_day10_Graph)
print(Colony3_day10_Clust[1],sorted(Colony3_day10_Clust[2]))
par_colony3['day 10']=Colony3_day10_Clust[0]

# """
# Do a comparison for points
# 
# To compare the changing in the colonies during some days, let's analyze and assign the 
# work to each group.
# 
# The cleaners are the ones with less interaction inside their group, and more outside it.
# The nurses are the ones with highest number of interaction inside their group.
# """


# In this section the changes of community are computed, firstly assigning each vertex to its community,
# then creating the condensed version of the graph to assign each community to a specific work.
# Finally work_changes function is applied to count the each type of change.
# com_colX_dY is a dictionary where the keys are the nodes, the values are the label of community
# graph_cX_dY is a graph where each node is a community and the (intra and extra) edges indicates the presence of original edges between communities, the weights are the amount of edges in the original graph
# com_cX_dY_assigned is a dictionary where the keys are the nodes, the values are the name of the assigned work
# changes is a dictionary where the keys are the pair of compared datasets and the values are the vector containing all the changes divided by type
####################################################################################################

changes={}

############################# COLONY 1 WORK ASSIGNMENTS  and CHANGES ###############################

com_col1_d1=par2com(par_colony1['day 1'])
graph_c1_d1_red=new_network(Colony1_day1_Graph, com_col1_d1, par_colony1['day 1'])
graph_c1_d1_red.edges(data=True)
com_c1_d1_assigned=work_assign(graph_c1_d1_red, com_col1_d1)

com_col1_d5=par2com(par_colony1['day 5'])
graph_c1_d5_red=new_network(Colony1_day5_Graph, com_col1_d5, par_colony1['day 5'])
graph_c1_d5_red.edges(data=True)
com_c1_d5_assigned=work_assign(graph_c1_d5_red, com_col1_d5)

com_col1_d10=par2com(par_colony1['day 10'])
graph_c1_d10_red=new_network(Colony1_day10_Graph, com_col1_d10, par_colony1['day 10'])
graph_c1_d10_red.edges(data=True)
com_c1_d10_assigned=work_assign(graph_c1_d10_red, com_col1_d10)

changes['c1 d1 d5']=work_changes(com_c1_d1_assigned,com_c1_d5_assigned)
changes['c1 d5 d10']=work_changes(com_c1_d5_assigned,com_c1_d10_assigned)

############################# COLONY 2 WORK ASSIGNMENTS and CHANGES ###############################

com_col2_d1=par2com(par_colony2['day 1'])
graph_c2_d1_red=new_network(Colony2_day1_Graph, com_col2_d1, par_colony2['day 1'])
graph_c2_d1_red.edges(data=True)
com_c2_d1_assigned=work_assign(graph_c2_d1_red, com_col2_d1)

com_col2_d5=par2com(par_colony2['day 5'])
graph_c2_d5_red=new_network(Colony2_day5_Graph, com_col2_d5, par_colony2['day 5'])
graph_c2_d5_red.edges(data=True)
com_c2_d5_assigned=work_assign(graph_c2_d5_red, com_col2_d5)

com_col2_d10=par2com(par_colony2['day 10'])
graph_c2_d10_red=new_network(Colony2_day10_Graph, com_col2_d10, par_colony2['day 10'])
graph_c2_d10_red.edges(data=True)
com_c2_d10_assigned=work_assign(graph_c2_d10_red, com_col2_d10)

changes['c2 d1 d5']=work_changes(com_c2_d1_assigned,com_c2_d5_assigned)
changes['c2 d5 d10']=work_changes(com_c2_d5_assigned,com_c2_d10_assigned)

############################# COLONY 3 WORK ASSIGNMENTS and CHANGES ###############################

com_col3_d1=par2com(par_colony3['day 1'])
graph_c3_d1_red=new_network(Colony3_day1_Graph, com_col3_d1, par_colony3['day 1'])
graph_c3_d1_red.edges(data=True)
com_c3_d1_assigned=work_assign(graph_c3_d1_red, com_col3_d1)

com_col3_d5=par2com(par_colony3['day 5'])
graph_c3_d5_red=new_network(Colony3_day5_Graph, com_col3_d5, par_colony3['day 5'])
graph_c3_d5_red.edges(data=True)
com_c3_d5_assigned=work_assign(graph_c3_d5_red, com_col3_d5)

com_col3_d10=par2com(par_colony3['day 10'])
graph_c3_d10_red=new_network(Colony3_day10_Graph, com_col3_d10, par_colony3['day 10'])
graph_c3_d10_red.edges(data=True)
com_c3_d10_assigned=work_assign(graph_c3_d10_red, com_col3_d10)

changes['c3 d1 d5']=work_changes(com_c3_d1_assigned,com_c3_d5_assigned)
changes['c3 d5 d10']=work_changes(com_c3_d5_assigned,com_c3_d10_assigned)

#It is also produced a vector containing the sum of all the changes, divided by type of works
changes['tot']=[0,0,0,0,0,0]
for day in changes.keys():
  if day!='tot':
    for count in range(0,len(changes[day])):
      changes['tot'][count]+=changes[day][count]
####################################################################################################
# """ 
# Node degree computation for the 'Nurse' nodes
# """

# For each colony and for each day a candidate queen is identified as the Nurse with highest
# degree inside its colony, then the three candidates for each colony are compared using the mean
# degree value. Finlly the node with highest mean degree value among these three is saved as queen
# of the colony
queen_c1=list()
queen_c1.append(find_queen(com_col1_d1, Colony1_day1_Graph)[0])
queen_c1.append(find_queen(com_col1_d5, Colony1_day5_Graph)[0])
queen_c1.append(find_queen(com_col1_d10, Colony1_day10_Graph)[0])

queen1=queen_comp(queen_c1, Colony1_day1_Graph, Colony1_day5_Graph, Colony1_day10_Graph)

queen_c2=list()
queen_c2.append(find_queen(com_col2_d1, Colony2_day1_Graph)[0])
queen_c2.append(find_queen(com_col2_d5, Colony2_day5_Graph)[0])
queen_c2.append(find_queen(com_col2_d10, Colony2_day10_Graph)[0])

queen2=queen_comp(queen_c2, Colony2_day1_Graph, Colony2_day5_Graph, Colony2_day10_Graph)

queen_c3=list()
queen_c3.append(find_queen(com_col3_d1, Colony3_day1_Graph)[0])
queen_c3.append(find_queen(com_col3_d5, Colony3_day5_Graph)[0])
queen_c3.append(find_queen(com_col3_d10, Colony3_day10_Graph)[0])

queen3=queen_comp(queen_c3, Colony3_day1_Graph, Colony3_day5_Graph, Colony3_day10_Graph)
